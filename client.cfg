# TODO: Maybe deploy this configuration with ansible instead of checking it in to avoid installation specific values.

[hadoop]
version = apache1

[core]
logging_conf_file=logging.cfg
hdfs-tmp-dir=/tmp/luigi/partial

[map-reduce]
engine = hadoop
marker = s3://edx-analytics-scratch/marker/

[event-logs]
# The environment represents a consistent key space where joins can be performed without fear of consistency issues
environment = prod
expand_interval = 2 days
source = s3://edx-all-tracking-logs/

# TODO: Figure out a better way to store a per-environment mapping like this.
[environment:prod]
pattern = .*?prod-(?:edx(?:app)?|worker)-\d{3}/tracking.log-(?P<date>\d{8}).*\.gz

[environment:edge]
pattern = .*?prod-edge-(?:edx(?:app)?|worker)-\d{3}/tracking.log-(?P<date>\d{8}).*\.gz

[event-export]
config = s3://edx-analytics-data/export-config.yaml
gpg_key_dir = s3://edx-analytics-data/gpg-keys/
gpg_master_key = analytics@edx.org

[manifest]
threshold = 500
input_format = oddjob.ManifestTextInputFormat
# TODO: Operationalize the build process for this jar.
lib_jar = s3://edx-analytics-packages/oddjob-1.0.1-standalone.jar
path = s3://edx-analytics-scratch/manifest/
